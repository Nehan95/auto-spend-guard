#!/usr/bin/env python3
"""
Test script for Token Tracking in LangGraph Workflow
Demonstrates token counting and cost estimation capabilities
"""

from langgraph_workflow import SpendAnalyzerWorkflow

def test_token_tracking():
    """Test the token tracking functionality"""
    print("üß™ TESTING TOKEN TRACKING IN LANGRAPH WORKFLOW")
    print("=" * 60)
    
    try:
        # Initialize workflow
        workflow = SpendAnalyzerWorkflow()
        print("‚úÖ Workflow initialized successfully!")
        
        # Test question to trigger the workflow
        test_question = "What are our AWS costs and which services are most expensive?"
        print(f"\nüìù Test Question: {test_question}")
        print("-" * 50)
        
        # Run the workflow
        print("üöÄ Running workflow with token tracking...")
        result = workflow.run({
            "question": test_question
        })
        
        print("\n‚úÖ Workflow completed successfully!")
        
        # Display token metrics
        if "latency_metrics" in result:
            print("\nüî§ TOKEN USAGE BREAKDOWN:")
            print("=" * 40)
            
            # Question Classification Tokens
            qc_metrics = result["latency_metrics"].get("question_classification", {})
            if qc_metrics:
                print(f"\nüîç Question Classification:")
                print(f"   ‚Ä¢ Input Tokens: {qc_metrics.get('input_tokens', 0)}")
                print(f"   ‚Ä¢ Output Tokens: {qc_metrics.get('output_tokens', 0)}")
                print(f"   ‚Ä¢ Total Tokens: {qc_metrics.get('total_tokens', 0)}")
            
            # Response Generation Tokens
            rg_metrics = result["latency_metrics"].get("response_generation", {})
            if rg_metrics:
                print(f"\nüîç Response Generation:")
                print(f"   ‚Ä¢ Input Tokens: {rg_metrics.get('input_tokens', 0)}")
                print(f"   ‚Ä¢ Output Tokens: {rg_metrics.get('output_tokens', 0)}")
                print(f"   ‚Ä¢ Total Tokens: {rg_metrics.get('total_tokens', 0)}")
            
            # Calculate total tokens and cost
            total_input = sum([
                qc_metrics.get('input_tokens', 0),
                rg_metrics.get('input_tokens', 0)
            ])
            total_output = sum([
                qc_metrics.get('output_tokens', 0),
                rg_metrics.get('output_tokens', 0)
            ])
            total_tokens = total_input + total_output
            
            print(f"\nüìä TOKEN SUMMARY:")
            print(f"   ‚Ä¢ Total Input Tokens: {total_input}")
            print(f"   ‚Ä¢ Total Output Tokens: {total_output}")
            print(f"   ‚Ä¢ Total Tokens: {total_tokens}")
            
            # Cost estimation for different models
            print(f"\nüí∞ COST ESTIMATION:")
            print(f"   ‚Ä¢ GPT-3.5-turbo: ${(total_tokens * 0.000002):.4f}")
            print(f"   ‚Ä¢ GPT-4: ${(total_tokens * 0.00003):.4f}")
            print(f"   ‚Ä¢ GPT-4-turbo: ${(total_tokens * 0.00001):.4f}")
            
        else:
            print("\n‚ùå No latency metrics found in result")
        
        # Display full latency metrics if available
        if hasattr(workflow, 'display_latency_metrics'):
            print("\n" + "="*60)
            print("üìä FULL LATENCY & TOKEN METRICS")
            print("="*60)
            workflow.display_latency_metrics(result)
        
        print("\n" + "="*60)
        print("üéâ Token tracking test completed!")
        print("\nüöÄ Token Tracking Features:")
        print("   ‚Ä¢ Real-time token counting for all LLM operations")
        print("   ‚Ä¢ Input vs. output token breakdown")
        print("   ‚Ä¢ Cost estimation for multiple GPT models")
        print("   ‚Ä¢ Performance optimization insights")
        print("   ‚Ä¢ Budget planning and cost control")
        
    except Exception as e:
        print(f"‚ùå Error testing token tracking: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_token_tracking()
